{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시간 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calc_fps(img_cnt, start_time, end_time):\n",
    "    total_time = end_time - start_time\n",
    "    fps = img_cnt / total_time\n",
    "    \n",
    "    print(\"FPS:\", fps, \", Total:\", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectron2 Keypoints rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 코 (Nose)\n",
    "2. 왼쪽 눈 (Left Eye)\n",
    "3. 오른쪽 눈 (Right Eye)\n",
    "4. 왼쪽 귀 (Left Ear)\n",
    "5. 오른쪽 귀 (Right Ear)\n",
    "\n",
    "6. 왼쪽 어깨 (Left Shoulder)\n",
    "7. 오른쪽 어깨 (Right Shoulder)\n",
    "\n",
    "8. 왼쪽 팔꿈치 (Left Elbow)\n",
    "9. 오른쪽 팔꿈치 (Right Elbow)\n",
    "10. 왼쪽 손목 (Left Wrist)\n",
    "11. 오른쪽 손목 (Right Wrist)\n",
    "\n",
    "12. 왼쪽 엉덩이 (Left Hip)\n",
    "13. 오른쪽 엉덩이 (Right Hip)\n",
    "\n",
    "14. 왼쪽 무릎 (Left Knee)\n",
    "15. 오른쪽 무릎 (Right Knee)\n",
    "16. 왼쪽 발목 (Left Ankle)\n",
    "17. 오른쪽 발목 (Right Ankle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\705-18\\anaconda3\\envs\\detectron\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  \n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE='cuda' #만약 gpu를 사용한다면 ‘cuda’로 수정\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\705-18\\anaconda3\\envs\\detectron\\lib\\site-packages\\detectron2\\structures\\image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "c:\\Users\\705-18\\anaconda3\\envs\\detectron\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\705-18\\anaconda3\\envs\\detectron\\lib\\site-packages\\detectron2\\structures\\keypoints.py:224: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  y_int = (pos - x_int) // w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 7.105010271072388, Total: 4.2223725027032195\n"
     ]
    }
   ],
   "source": [
    "# 이미지들이 저장된 폴더 경로\n",
    "image_folder = 'mAP_dataset\\images\\\\train'\n",
    "\n",
    "# 이미지 파일 목록을 불러오기\n",
    "image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "# 시작시간\n",
    "start_time = time.time()\n",
    "\n",
    "# 이미지 처리 루프\n",
    "for image_file in image_files:\n",
    "    # 이미지 경로 생성\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    # 이미지 읽어오기\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    # Detectron2를 이용한 관절 검출\n",
    "    keypoints = predictor(frame)['instances'].get_fields()['pred_keypoints'][0]\n",
    "\n",
    "# 종료 시간\n",
    "end_time = time.time()\n",
    "calc_fps(len(image_files), start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO-pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검출 관절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0번 == 코   \n",
    "1번 == 오른쪽 눈   \n",
    "2번 == 왼쪽 눈   \n",
    "3번 == 오른쪽 귀   \n",
    "4번 == 왼쪽 귀   \n",
    "5번 == 오른쪽 어깨   \n",
    "6번 == 왼쪽 어깨   \n",
    "7번 == 오른쪽 팔꿈치   \n",
    "8번 == 왼쪽 팔꿈치   \n",
    "9번 == 오른쪽 손목   \n",
    "10번 == 왼쪽 손목   \n",
    "11번 == 오른쪽 골반   \n",
    "12번 == 왼쪽 골반   \n",
    "13번 == 오른쪽 무릎   \n",
    "14번 == 왼쪽 무릎   \n",
    "15번 == 오른쪽 발   \n",
    "16번 == 왼쪽 발   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002414.jpg: 384x640 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 12.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002418.jpg: 384x640 1 person, 31.3ms\n",
      "Speed: 0.1ms preprocess, 31.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002425.jpg: 384x640 1 person, 16.3ms\n",
      "Speed: 0.0ms preprocess, 16.3ms inference, 17.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002429.jpg: 384x640 1 person, 31.0ms\n",
      "Speed: 0.0ms preprocess, 31.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002435.jpg: 384x640 1 person, 35.3ms\n",
      "Speed: 0.0ms preprocess, 35.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002438.jpg: 384x640 1 person, 31.8ms\n",
      "Speed: 0.0ms preprocess, 31.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002440.jpg: 384x640 1 person, 33.2ms\n",
      "Speed: 0.0ms preprocess, 33.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002442.jpg: 384x640 1 person, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 18.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002444.jpg: 384x640 1 person, 14.8ms\n",
      "Speed: 0.3ms preprocess, 14.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002446.jpg: 384x640 1 person, 40.9ms\n",
      "Speed: 0.0ms preprocess, 40.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002448.jpg: 384x640 1 person, 39.7ms\n",
      "Speed: 0.0ms preprocess, 39.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002450.jpg: 384x640 1 person, 32.0ms\n",
      "Speed: 0.0ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002452.jpg: 384x640 1 person, 19.8ms\n",
      "Speed: 2.6ms preprocess, 19.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002454.jpg: 384x640 1 person, 29.0ms\n",
      "Speed: 0.0ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002456.jpg: 384x640 1 person, 24.5ms\n",
      "Speed: 13.7ms preprocess, 24.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002458.jpg: 384x640 1 person, 30.0ms\n",
      "Speed: 0.0ms preprocess, 30.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002460.jpg: 384x640 1 person, 22.1ms\n",
      "Speed: 2.2ms preprocess, 22.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002462.jpg: 384x640 1 person, 31.1ms\n",
      "Speed: 0.0ms preprocess, 31.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002464.jpg: 384x640 1 person, 21.8ms\n",
      "Speed: 2.2ms preprocess, 21.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002466.jpg: 384x640 1 person, 30.9ms\n",
      "Speed: 0.0ms preprocess, 30.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002468.jpg: 384x640 1 person, 22.4ms\n",
      "Speed: 4.0ms preprocess, 22.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002470.jpg: 384x640 2 persons, 37.0ms\n",
      "Speed: 0.0ms preprocess, 37.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002472.jpg: 384x640 1 person, 25.0ms\n",
      "Speed: 0.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002474.jpg: 384x640 3 persons, 32.0ms\n",
      "Speed: 0.0ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002477.jpg: 384x640 3 persons, 11.9ms\n",
      "Speed: 2.0ms preprocess, 11.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002480.jpg: 384x640 4 persons, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002483.jpg: 384x640 2 persons, 22.1ms\n",
      "Speed: 2.2ms preprocess, 22.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002487.jpg: 384x640 1 person, 29.9ms\n",
      "Speed: 0.0ms preprocess, 29.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002506.jpg: 384x640 1 person, 21.4ms\n",
      "Speed: 3.0ms preprocess, 21.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 c:\\Users\\705-18\\Desktop\\  \\mAP_dataset\\images\\train\\S63_DATA2_FP_L1_D2023-08-25-13-39_001_002509.jpg: 384x640 1 person, 20.1ms\n",
      "Speed: 0.0ms preprocess, 20.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "FPS: 16.435508627232927 , Total: 1.8253161907196045\n"
     ]
    }
   ],
   "source": [
    "# 이미지들이 저장된 폴더 경로\n",
    "image_folder = 'mAP_dataset\\images\\\\train'\n",
    "\n",
    "# 이미지 파일 목록을 불러오기\n",
    "image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "# 시작시간\n",
    "start_time = time.time()\n",
    "\n",
    "# 이미지 처리 루프\n",
    "for image_file in image_files:\n",
    "    # 이미지 경로 생성\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    # 이미지 읽어오기\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"이미지 로드에 실패했습니다: {image_file}\")\n",
    "        continue\n",
    "\n",
    "    # yolo 관절 검출\n",
    "    results = model(os.path.join(image_folder, image_file))\n",
    "\n",
    "# 종료 시간\n",
    "end_time = time.time()\n",
    "calc_fps(len(image_files), start_time, end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
